# MultiServe


## ğŸš€ MultiServe: Efficient Tool-Augmented Inference with Collaborative LLMs

![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)
![GPU](https://img.shields.io/badge/GPU-NVIDIA%20H100-green)
![LLM](https://img.shields.io/badge/Model-LLaMA3%2070B-purple)

> A collaborative LLM inference framework that combines speculative inference, dynamic cache scheduling, and tool augmentation â€“ for blazing-fast, memory-efficient, and secure multi-agent reasoning ğŸ”§ğŸ§ âš¡ï¸
>
> *Refining is in progress and will be open source soon.*

---

## âœ¨ Features

- ğŸ¤– **Dual-Model Collaboration**  
  Leverage a lightweight **Speculative Model** to predict tool calls and prefetch responses, while a heavyweight **Decode Model** generates high-fidelity outputs.

- ğŸ“¦ **Fine-Grained KV Cache Management**  
  Smart cache partitioning, swapping, and prefix reuse using block-based scheduling and Radix Attention-inspired hashing.

- ğŸ” **Tool-Augmented Speculative Decoding**  
  Seamlessly integrate external tools (e.g., calculators, search engines) into the reasoning pipeline with asynchronous orchestration.

- ğŸ›¡ï¸ **Resource-Aware Scheduling**  
  Dynamically reallocate GPU memory based on queue pressure and model load, improving throughput and robustness under heavy concurrency.

- ğŸ“ˆ **SOTA Performance**  
  Achieves **2Ã—â€“3Ã— speedups** over existing LLM serving frameworks like vLLM and InferCept on benchmarks like GSM8K and HotpotQA.

---

## ğŸ“Š Benchmark Results

| Task     | Latency â†“ | Throughput â†‘ | Accuracy |
| -------- | --------- | ------------ | -------- |
| GSM8K    | 2.7Ã—      | +180%        | âœ…        |
| HotpotQA | 1.3Ã—      | +95%         | âœ…        |

---

## ğŸ”§ Getting Started

```bash
# Clone the repo
git clone https://github.com/UG-Thesis-MultiServe/MultiServe.git
cd MultiServe

# Install dependencies
conda env create -f environment.yml
conda activate multiserve

# Run server
python launch.py --config configs/llama3-70b.yaml
```

> ğŸ“Œ Note: Tested on 4Ã— NVIDIA H100 GPUs with PyTorch 2.1, CUDA 12.2, and vLLM backend.

---

## ğŸ“œ License

This project is licensed under the **Apache License 2.0**.
Feel free to use, modify, and redistribute with proper attribution. â¤ï¸

---

## ğŸ™Œ Acknowledgements

Built on top of ideas from [vLLM](https://github.com/vllm-project/vllm), [FlexGen](https://github.com/FMInference/FlexGen), and inspired by ReAct and speculative decoding research.
Special thanks to the faculty and research mentors at **USTC** and **UC San Diego**. ğŸ“ğŸ‡¨ğŸ‡³
